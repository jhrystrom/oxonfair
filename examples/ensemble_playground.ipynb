{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from joblib import Memory\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    ModelOutput,  # or just use dict if not subclassing\n",
    ")\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "CACHE_DIR = Path().cwd().parent / \".cache\"\n",
    "if not CACHE_DIR.exists():\n",
    "    CACHE_DIR.mkdir()\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(\n",
    "        predictions=predictions, references=labels.astype(int).reshape(-1)\n",
    "    )\n",
    "\n",
    "\n",
    "# 8. Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"multilabel_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load tokenizer\n",
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 25621/25621 [00:02<00:00, 10925.88 examples/s]\n",
      "Map: 100%|██████████| 6103/6103 [00:01<00:00, 5337.16 examples/s] \n",
      "/var/folders/bd/m7_xt1cd097bwm56ny1027480000gn/T/ipykernel_67989/342344130.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='9609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  64/9609 00:20 < 51:25, 3.09 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_full_data():\n",
    "    english_hatespeech = Path().cwd().parent / \"hatespeech-data\" / \"split\" / \"English\"\n",
    "    all_data = list(english_hatespeech.glob(\"*.tsv\"))\n",
    "    return (\n",
    "        pl.DataFrame(\n",
    "            pd.concat([pd.read_csv(f, sep=\"\\t\") for f in all_data]).drop(\n",
    "                columns=[\"city\", \"state\", \"country\", \"date\"]\n",
    "            )\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"gender\").replace(\"x\", None).cast(pl.Int8),\n",
    "            pl.col(\"age\").replace(\"x\", None).cast(pl.Int8),\n",
    "            pl.col(\"ethnicity\").replace(\"x\", None).cast(pl.Int8),\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .rename({\"label\": \"target\"})\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    features: pl.DataFrame, labels: pl.Series, feature_names: list[str] | None = None\n",
    ") -> Dataset:\n",
    "    if feature_names is None:\n",
    "        feature_names = features.columns\n",
    "    feature_dict = {feature: features[feature].to_list() for feature in feature_names}\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            **feature_dict,\n",
    "            \"target\": labels.to_list(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def tokenize(text: str) -> dict[str, Any]:\n",
    "    return tokenizer(text, truncation=True)\n",
    "\n",
    "\n",
    "def preprocess_simple(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    tokenized = tokenize(example[\"text\"])\n",
    "    labels = [float(example[key]) for key in [\"target\", \"gender\", \"age\", \"ethnicity\"]]\n",
    "    assert len(labels) == 4\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def compute_loss_func(\n",
    "    outputs: ModelOutput | dict,\n",
    "    labels: torch.Tensor,\n",
    "    num_items_in_batch: int,  # noqa: ARG001\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Custom loss function for HuggingFace Trainer:\n",
    "    - Binary log loss for the first element\n",
    "    - Squared loss (MSE) for the remaining elements\n",
    "\n",
    "    Args:\n",
    "        outputs: ModelOutput or dict containing 'logits' of shape (batch_size, num_outputs)\n",
    "        labels: Tensor of shape (batch_size, num_outputs), ground-truth labels\n",
    "        num_items_in_batch: Total number of items in the accumulated batch (unused here)\n",
    "        num_classification_labels: Number of non-group based classification labels (default: 2)\n",
    "\n",
    "    Returns:\n",
    "        Scalar tensor representing the combined loss\n",
    "    \"\"\"\n",
    "    logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n",
    "\n",
    "    log_loss = F.binary_cross_entropy_with_logits(logits[:, :1], labels[:, :1])\n",
    "\n",
    "    # Regression loss (MSE) for remaining outputs\n",
    "    if logits.shape[1] > 1:\n",
    "        mse_loss = F.mse_loss(logits[:, 1:], labels[:, 1:])\n",
    "        loss = log_loss + mse_loss\n",
    "    else:\n",
    "        loss = log_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "combined = get_full_data()\n",
    "\n",
    "# 5. Prepare data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# 6. Metrics function\n",
    "# 7. Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=4,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "\n",
    "\n",
    "K = 5\n",
    "gss = GroupShuffleSplit(n_splits=K, train_size=0.8, random_state=110)\n",
    "all_features = combined.drop(\"target\", \"tid\", \"uid\")\n",
    "all_labels = combined[\"target\"]\n",
    "all_users = combined[\"uid\"]\n",
    "\n",
    "trainers = []\n",
    "for train_index, test_index in gss.split(all_features, all_labels, groups=all_users):\n",
    "    train_features = all_features[train_index]\n",
    "    train_labels = all_labels[train_index]\n",
    "    train_groups = all_users[train_index]\n",
    "    test_features = all_features[test_index]\n",
    "    test_labels = all_labels[test_index]\n",
    "\n",
    "    inner_gss = GroupShuffleSplit(n_splits=2, train_size=0.8, random_state=110)\n",
    "    # nested cross-validation\n",
    "    for inner_train_index, validation_index in inner_gss.split(\n",
    "        train_features, train_labels, groups=train_groups\n",
    "    ):\n",
    "        inner_train_features = train_features[inner_train_index]\n",
    "        inner_train_labels = train_labels[inner_train_index]\n",
    "        inner_train_groups = train_groups[inner_train_index]\n",
    "        inner_validation_features = train_features[validation_index]\n",
    "        inner_validation_labels = train_labels[validation_index]\n",
    "        inner_validation_groups = train_groups[validation_index]\n",
    "        assert inner_validation_groups.shape[0] == validation_index.shape[0]\n",
    "\n",
    "        train_dataset = create_dataset(\n",
    "            inner_train_features,\n",
    "            inner_train_labels,\n",
    "        ).map(preprocess_simple)\n",
    "\n",
    "        validation_dataset = create_dataset(\n",
    "            inner_validation_features,\n",
    "            inner_validation_labels,\n",
    "        ).map(preprocess_simple)\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=validation_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_loss_func=compute_loss_func,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "        trainers.append(trainer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"hashtag totalblacktv 1 don't ask me to think ... i just want to entertain ! hashtag hashtag url\",\n",
       " 'gender': 0,\n",
       " 'age': 0,\n",
       " 'ethnicity': 1,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classes = [\"pos\", \"neg\"]\n",
    "# Assuming Target column contains the classes we want to one-hot encode\n",
    "target_classes = [\"1\", \"2\"]\n",
    "all_classes = sentiment_classes + target_classes\n",
    "# Create dictionaries for mapping\n",
    "sentiment_class2id = {\n",
    "    class_: class_id for class_id, class_ in enumerate(sentiment_classes)\n",
    "}\n",
    "target_class2id = {class_: class_id for class_id, class_ in enumerate(target_classes)}\n",
    "# All class mappings (sentiment + targets)\n",
    "all_classes = sentiment_classes + target_classes\n",
    "class2id = {class_: class_id for class_id, class_ in enumerate(all_classes)}\n",
    "id2class = {class_id: class_ for class_, class_id in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.39380065,  0.516599  ,  0.4008736 ,  0.28612477],\n",
       "       [-0.6755451 ,  0.52855605,  0.2793328 ,  0.2614335 ],\n",
       "       [-0.1146356 ,  0.52998024,  0.32771546,  0.3220777 ],\n",
       "       ...,\n",
       "       [-0.67850834,  0.5743937 ,  0.35765246,  0.3002295 ],\n",
       "       [-1.4025906 ,  0.5479808 ,  0.35175514,  0.24082464],\n",
       "       [-1.0753162 ,  0.5114276 ,  0.4914688 ,  0.47242028]],\n",
       "      dtype=float32), label_ids=array([[0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 1., 1.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [0., 1., 1., 1.]], dtype=float32), metrics={'test_loss': 0.6549593210220337, 'test_accuracy': 0.5945027035883992, 'test_f1': 0.6906852482579758, 'test_precision': 0.5466686451995845, 'test_recall': 0.9377227218734091, 'test_runtime': 54.6413, 'test_samples_per_second': 111.692, 'test_steps_per_second': 13.964})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/m7_xt1cd097bwm56ny1027480000gn/T/ipykernel_49715/1937891243.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='819' max='819' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [819/819 03:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439806</td>\n",
       "      <td>0.530275</td>\n",
       "      <td>0.530152</td>\n",
       "      <td>0.371145</td>\n",
       "      <td>0.927523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.432666</td>\n",
       "      <td>0.561206</td>\n",
       "      <td>0.546587</td>\n",
       "      <td>0.387779</td>\n",
       "      <td>0.925688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.504109</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>0.553028</td>\n",
       "      <td>0.394295</td>\n",
       "      <td>0.925688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=819, training_loss=0.3579169612227779, metrics={'train_runtime': 204.6023, 'train_samples_per_second': 31.964, 'train_steps_per_second': 4.003, 'total_flos': 143747311387200.0, 'train_loss': 0.3579169612227779, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Function to make predictions\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(\n",
    "        model.device\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    probs = sigmoid(logits)[0]\n",
    "\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "\n",
    "    # Get sentiment prediction\n",
    "    sentiment_idx = np.argmax(probs[: len(sentiment_classes)])\n",
    "    sentiment = sentiment_classes[sentiment_idx]\n",
    "\n",
    "    # Get target predictions (can be multiple)\n",
    "    target_predictions = []\n",
    "    for i, val in enumerate(predictions[len(sentiment_classes) :]):\n",
    "        if val == 1:\n",
    "            target_predictions.append(target_classes[i])\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"targets\": target_predictions,\n",
    "        \"probabilities\": {id2class[i]: float(probs[i]) for i in range(len(probs))},\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
