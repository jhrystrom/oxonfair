{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /VData/resources/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Thu Mar 20 16:34:30 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /VData/resources/huggingface/modules/evaluate_modules/metrics/evaluate-metric--precision/155d3220d6cd4a6553f12da68eeb3d1f97cf431206304a4bc6e2d564c29502e9 (last modified on Thu Mar 20 16:34:31 2025) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /VData/resources/huggingface/modules/evaluate_modules/metrics/evaluate-metric--recall/11f90e583db35601050aed380d48e83202a896976b9608432fba9244fb447f24 (last modified on Thu Mar 20 16:34:31 2025) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Any, TypedDict\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from loguru import logger\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TextClassificationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    ModelOutput,  # or just use dict if not subclassing\n",
    ")\n",
    "\n",
    "import oxonfair\n",
    "from oxonfair import group_metrics as gm\n",
    "\n",
    "\n",
    "class FairnessMetrics(TypedDict):\n",
    "    equal_opportunity: float\n",
    "    min_recall: float\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "\n",
    "\n",
    "def calculate_metrics(\n",
    "    test_groups: pl.Series, test_labels: pl.Series, predictions: list[str] | np.ndarray\n",
    ") -> FairnessMetrics:\n",
    "    groups = test_groups.to_numpy()\n",
    "    preds0 = np.array(predictions)[groups == 0]\n",
    "    preds1 = np.array(predictions)[groups == 1]\n",
    "    labels0 = test_labels.to_numpy()[groups == 0]\n",
    "    labels1 = test_labels.to_numpy()[groups == 1]\n",
    "\n",
    "    recall1 = recall_score(y_true=labels1, y_pred=preds1)\n",
    "    recall0 = recall_score(y_true=labels0, y_pred=preds0)\n",
    "\n",
    "    min_recall = min(recall0, recall1)\n",
    "    equal_opportunity = abs(recall1 - recall0)\n",
    "    return {\n",
    "        \"min_recall\": min_recall,\n",
    "        \"equal_opportunity\": equal_opportunity,\n",
    "        \"accuracy\": accuracy_score(y_true=test_labels.to_numpy(), y_pred=predictions),\n",
    "        \"precision\": precision_score(y_true=test_labels, y_pred=predictions),\n",
    "        \"recall\": recall_score(y_true=test_labels, y_pred=predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "CACHE_DIR = Path().cwd().parent / \".cache\"\n",
    "if not CACHE_DIR.exists():\n",
    "    CACHE_DIR.mkdir()\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(\n",
    "        predictions=predictions, references=labels.astype(int).reshape(-1)\n",
    "    )\n",
    "\n",
    "\n",
    "# 8. Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"multilabel_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load tokenizer\n",
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 6131/6131 [00:00<00:00, 6333.72 examples/s]\n",
      "Map: 100%|██████████| 1223/1223 [00:00<00:00, 6300.71 examples/s]\n",
      "/tmp/ipykernel_3439903/3254392779.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 01:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.659002</td>\n",
       "      <td>0.680294</td>\n",
       "      <td>0.717281</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.859619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.614140</td>\n",
       "      <td>0.681112</td>\n",
       "      <td>0.721826</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.876950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.623296</td>\n",
       "      <td>0.682339</td>\n",
       "      <td>0.724370</td>\n",
       "      <td>0.613213</td>\n",
       "      <td>0.884749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 6118/6118 [00:01<00:00, 5433.48 examples/s]\n",
      "Map: 100%|██████████| 1236/1236 [00:00<00:00, 9028.29 examples/s]\n",
      "/tmp/ipykernel_3439903/3254392779.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 01:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653099</td>\n",
       "      <td>0.673544</td>\n",
       "      <td>0.702324</td>\n",
       "      <td>0.596118</td>\n",
       "      <td>0.854578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.645271</td>\n",
       "      <td>0.677589</td>\n",
       "      <td>0.711336</td>\n",
       "      <td>0.596236</td>\n",
       "      <td>0.881508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>0.658186</td>\n",
       "      <td>0.677994</td>\n",
       "      <td>0.712635</td>\n",
       "      <td>0.596014</td>\n",
       "      <td>0.885996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 5105/5105 [00:00<00:00, 6895.81 examples/s]\n",
      "Map: 100%|██████████| 2249/2249 [00:00<00:00, 9222.75 examples/s]\n",
      "/tmp/ipykernel_3439903/3254392779.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 01:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.779013</td>\n",
       "      <td>0.801676</td>\n",
       "      <td>0.706648</td>\n",
       "      <td>0.926233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.748531</td>\n",
       "      <td>0.735883</td>\n",
       "      <td>0.776440</td>\n",
       "      <td>0.655962</td>\n",
       "      <td>0.951130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.732048</td>\n",
       "      <td>0.749444</td>\n",
       "      <td>0.783810</td>\n",
       "      <td>0.671156</td>\n",
       "      <td>0.941909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-09 10:11:24.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mDone training ensemble! Evaluating on test set\u001b[0m\n",
      "Map: 100%|██████████| 1526/1526 [00:00<00:00, 8866.54 examples/s]\n",
      "\u001b[32m2025-04-09 10:11:24.543\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m207\u001b[0m - \u001b[34m\u001b[1mEvaluating ensemble...\u001b[0m\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "\u001b[32m2025-04-09 10:12:28.130\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m214\u001b[0m - \u001b[34m\u001b[1mEvaluating first member...\u001b[0m\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "def majority_vote(lists: list[list[bool]]) -> list[bool]:\n",
    "    return [sum(sublist) > len(sublist) / 2 for sublist in lists]\n",
    "\n",
    "\n",
    "def convert_score(score: float, threshold: float = 0.5) -> bool:\n",
    "    return score > threshold\n",
    "\n",
    "\n",
    "def aggregate_scores(scores: list[list[dict]], threshold: float = 0.5) -> list[bool]:\n",
    "    num_preds = len(scores[0])\n",
    "    # Convert to list[(pred0, pred0, pred0), (pred1, ...]\n",
    "    final_preds = []\n",
    "    for pred_index in range(num_preds):\n",
    "        pred_list = []\n",
    "        for score_list in scores:\n",
    "            score_dict = score_list[pred_index]\n",
    "            pred_list.append(convert_score(score_dict[\"score\"], threshold=threshold))\n",
    "        final_preds.append(pred_list)\n",
    "    return majority_vote(final_preds)\n",
    "\n",
    "\n",
    "def max_index_by_key(lst: list[dict], key: str = \"score\"):\n",
    "    if not lst:\n",
    "        return None\n",
    "    return max(range(len(lst)), key=lambda i: lst[i][key])\n",
    "\n",
    "\n",
    "def ensemble_predict(texts: list[str], ensemble: list[Trainer]) -> list[int]:\n",
    "    device = ensemble[0].model.device  # Get device from first model\n",
    "    pipes = [\n",
    "        TextClassificationPipeline(\n",
    "            tokenizer=tokenizer,\n",
    "            model=trainer.model.to(device),\n",
    "            device=device,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for trainer in ensemble\n",
    "    ]\n",
    "    preds = [pipe(texts) for pipe in pipes]\n",
    "    return aggregate_scores(preds)\n",
    "\n",
    "\n",
    "def get_full_data():\n",
    "    english_hatespeech = Path().cwd().parent / \"hatespeech-data\" / \"split\" / \"English\"\n",
    "    all_data = list(english_hatespeech.glob(\"*.tsv\"))\n",
    "    return (\n",
    "        pl.DataFrame(\n",
    "            pd.concat([pd.read_csv(f, sep=\"\\t\") for f in all_data]).drop(\n",
    "                columns=[\"city\", \"state\", \"country\", \"date\"]\n",
    "            )\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"gender\").replace(\"x\", None).cast(pl.Int8),\n",
    "            pl.col(\"age\").replace(\"x\", None).cast(pl.Int8),\n",
    "            pl.col(\"ethnicity\").replace(\"x\", None).cast(pl.Int8),\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .rename({\"label\": \"target\"})\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    features: pl.DataFrame, labels: pl.Series, feature_names: list[str] | None = None\n",
    ") -> Dataset:\n",
    "    if feature_names is None:\n",
    "        feature_names = features.columns\n",
    "    feature_dict = {feature: features[feature].to_list() for feature in feature_names}\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            **feature_dict,\n",
    "            \"target\": labels.to_list(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def tokenize(text: str) -> dict[str, Any]:\n",
    "    return tokenizer(text, truncation=True)\n",
    "\n",
    "\n",
    "def preprocess_simple(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    tokenized = tokenize(example[\"text\"])\n",
    "    labels = [float(example[key]) for key in [\"target\", \"gender\"]]\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def compute_loss_func(\n",
    "    outputs: ModelOutput | dict,\n",
    "    labels: torch.Tensor,\n",
    "    num_items_in_batch: int,  # noqa: ARG001\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Custom loss function for HuggingFace Trainer:\n",
    "    - Binary log loss for the first element\n",
    "    - Squared loss (MSE) for the remaining elements\n",
    "\n",
    "    Args:\n",
    "        outputs: ModelOutput or dict containing 'logits' of shape (batch_size, num_outputs)\n",
    "        labels: Tensor of shape (batch_size, num_outputs), ground-truth labels\n",
    "        num_items_in_batch: Total number of items in the accumulated batch (unused here)\n",
    "        num_classification_labels: Number of non-group based classification labels (default: 2)\n",
    "\n",
    "    Returns:\n",
    "        Scalar tensor representing the combined loss\n",
    "    \"\"\"\n",
    "    logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n",
    "\n",
    "    log_loss = F.binary_cross_entropy_with_logits(logits[:, :1], labels[:, :1])\n",
    "\n",
    "    # Regression loss (MSE) for remaining outputs\n",
    "    if logits.shape[1] > 1:\n",
    "        mse_loss = F.mse_loss(logits[:, 1:], labels[:, 1:])\n",
    "        loss = log_loss + mse_loss\n",
    "    else:\n",
    "        loss = log_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "combined = get_full_data().sample(fraction=0.2)\n",
    "\n",
    "# 5. Prepare data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# 6. Metrics function\n",
    "# 7. Initialize model\n",
    "\n",
    "\n",
    "K = 3\n",
    "gss = GroupShuffleSplit(n_splits=K, train_size=0.8, random_state=110)\n",
    "all_features = combined.drop(\"target\", \"tid\", \"uid\", \"age\", \"ethnicity\")\n",
    "all_labels = combined[\"target\"]\n",
    "all_users = combined[\"uid\"]\n",
    "\n",
    "test_metrics = []\n",
    "all_metrics: list[pd.DataFrame] = []\n",
    "for iteration, (train_index, test_index) in enumerate(\n",
    "    gss.split(all_features, all_labels, groups=all_users)\n",
    "):\n",
    "    train_features = all_features[train_index]\n",
    "    train_labels = all_labels[train_index]\n",
    "    train_groups = all_users[train_index]\n",
    "    test_features = all_features[test_index]\n",
    "    test_labels = all_labels[test_index]\n",
    "\n",
    "    inner_gss = GroupShuffleSplit(n_splits=2, train_size=0.8, random_state=110)\n",
    "    # nested cross-validation\n",
    "    # Run oxonfair on an outer\n",
    "    # Min recall as a key metric for each test partition\n",
    "    # Key question: how big do we need to make the delta min recall to matter on the text\n",
    "    fair_ensemble = []\n",
    "    metrics = []\n",
    "    inner_gss = GroupShuffleSplit(n_splits=3, train_size=0.8, random_state=110)\n",
    "    for i, (inner_train_index, validation_index) in enumerate(\n",
    "        inner_gss.split(train_features, train_labels, groups=train_groups)\n",
    "    ):\n",
    "        inner_train_features = train_features[inner_train_index]\n",
    "        inner_train_labels = train_labels[inner_train_index]\n",
    "        inner_train_groups = train_groups[inner_train_index]\n",
    "        inner_validation_features = train_features[validation_index]\n",
    "        inner_validation_labels = train_labels[validation_index]\n",
    "        inner_validation_groups = train_groups[validation_index]\n",
    "        assert inner_validation_groups.shape[0] == validation_index.shape[0]\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_path,\n",
    "            num_labels=2,\n",
    "            problem_type=\"multi_label_classification\",\n",
    "        )\n",
    "\n",
    "        train_dataset = create_dataset(\n",
    "            inner_train_features,\n",
    "            inner_train_labels,\n",
    "        ).map(preprocess_simple)\n",
    "\n",
    "        validation_dataset = create_dataset(\n",
    "            inner_validation_features,\n",
    "            inner_validation_labels,\n",
    "        ).map(preprocess_simple)\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=validation_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_loss_func=compute_loss_func,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "        # Run oxonfair here? to merge heads etc.\n",
    "        val_output = trainer.predict(validation_dataset)\n",
    "        fpred = oxonfair.DeepFairPredictor(\n",
    "            inner_validation_labels.to_numpy(),\n",
    "            val_output.predictions,\n",
    "            groups=np.array(validation_dataset[\"gender\"]),\n",
    "        )\n",
    "        fpred.fit(gm.accuracy, gm.equal_opportunity, 0.02, grid_width=75)\n",
    "        fair_network = copy.deepcopy(trainer)\n",
    "        fair_network.model.classifier = fpred.merge_heads_pytorch(\n",
    "            fair_network.model.classifier\n",
    "        )\n",
    "        performance = fpred.evaluate().assign(classifier=i, metric_type=\"performance\")\n",
    "        fairness = fpred.evaluate_fairness(\n",
    "            metrics=gm.default_fairness_measures | {\"min_recall\": gm.recall.min}\n",
    "        ).assign(classifier=i, metric_type=\"fairness\")\n",
    "        metrics.append(pd.concat([performance, fairness]))\n",
    "        fair_ensemble.append(fair_network)\n",
    "    all_metrics.append(pd.concat(metrics).assign(iteration=iteration))\n",
    "    logger.info(\"Done training ensemble! Evaluating on test set\")\n",
    "    test_dataset = create_dataset(\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    ).map(preprocess_simple)\n",
    "    logger.debug(\"Evaluating ensemble...\")\n",
    "    ensemble_preds = ensemble_predict(\n",
    "        texts=test_dataset[\"text\"], ensemble=fair_ensemble\n",
    "    )\n",
    "    ensemble_metrics = calculate_metrics(\n",
    "        test_groups=test_features[\"gender\"],\n",
    "        test_labels=test_labels,\n",
    "        predictions=ensemble_preds,\n",
    "    )\n",
    "    logger.debug(\"Evaluating first member...\")\n",
    "    single_preds = ensemble_predict(\n",
    "        texts=test_dataset[\"text\"], ensemble=fair_ensemble[:1]\n",
    "    )\n",
    "    single_metrics = calculate_metrics(\n",
    "        test_groups=test_features[\"gender\"],\n",
    "        test_labels=test_labels,\n",
    "        predictions=single_preds,\n",
    "    )\n",
    "    single_df = pd.DataFrame([single_metrics]).assign(model_type=\"single\")\n",
    "    test_metric_df = pd.concat(\n",
    "        [single_df, pd.DataFrame([ensemble_metrics]).assign(model_type=\"ensemble\")]\n",
    "    ).assign(iteration=iteration)\n",
    "    test_metrics.append(test_metric_df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_recall</th>\n",
       "      <th>equal_opportunity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>model_type</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503650</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.787680</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>single</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521898</td>\n",
       "      <td>0.107849</td>\n",
       "      <td>0.825033</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.579661</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_recall  equal_opportunity  accuracy  precision    recall model_type  \\\n",
       "0    0.503650           0.009009  0.787680   0.898204  0.508475     single   \n",
       "0    0.521898           0.107849  0.825033   0.947368  0.579661   ensemble   \n",
       "\n",
       "   iteration  \n",
       "0          0  \n",
       "0          0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>updated</th>\n",
       "      <th>classifier</th>\n",
       "      <th>metric_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.838103</td>\n",
       "      <td>0.784137</td>\n",
       "      <td>0</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <td>0.822911</td>\n",
       "      <td>0.750949</td>\n",
       "      <td>0</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>0</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>0.666940</td>\n",
       "      <td>0.570736</td>\n",
       "      <td>0</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.865741</td>\n",
       "      <td>0.905844</td>\n",
       "      <td>0</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.727626</td>\n",
       "      <td>0.542802</td>\n",
       "      <td>0</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.905840</td>\n",
       "      <td>0.862754</td>\n",
       "      <td>0</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical Parity</th>\n",
       "      <td>0.117780</td>\n",
       "      <td>0.042029</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictive Parity</th>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equal Opportunity</th>\n",
       "      <td>0.111874</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Group Difference in False Negative Rate</th>\n",
       "      <td>0.111874</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized Odds</th>\n",
       "      <td>0.072012</td>\n",
       "      <td>0.015676</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conditional Use Accuracy</th>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.055882</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Group Difference in Accuracy</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.019168</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Treatment Equality</th>\n",
       "      <td>0.190827</td>\n",
       "      <td>0.074139</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimal Group Recall</th>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.531818</td>\n",
       "      <td>0</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.838997</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <td>0.817959</td>\n",
       "      <td>0.809933</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.777156</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>0.656419</td>\n",
       "      <td>0.675654</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.838164</td>\n",
       "      <td>0.920821</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.724426</td>\n",
       "      <td>0.655532</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.884706</td>\n",
       "      <td>0.880991</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical Parity</th>\n",
       "      <td>0.065764</td>\n",
       "      <td>0.057681</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictive Parity</th>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equal Opportunity</th>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.019629</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Group Difference in False Negative Rate</th>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.019629</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized Odds</th>\n",
       "      <td>0.022981</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conditional Use Accuracy</th>\n",
       "      <td>0.026546</td>\n",
       "      <td>0.020709</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Group Difference in Accuracy</th>\n",
       "      <td>0.008948</td>\n",
       "      <td>0.020596</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Treatment Equality</th>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimal Group Recall</th>\n",
       "      <td>0.706161</td>\n",
       "      <td>0.644550</td>\n",
       "      <td>1</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.817252</td>\n",
       "      <td>0.886172</td>\n",
       "      <td>2</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <td>0.769076</td>\n",
       "      <td>0.775249</td>\n",
       "      <td>2</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.625342</td>\n",
       "      <td>0.693046</td>\n",
       "      <td>2</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>0.508649</td>\n",
       "      <td>0.647475</td>\n",
       "      <td>2</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.577441</td>\n",
       "      <td>0.873112</td>\n",
       "      <td>2</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.681909</td>\n",
       "      <td>0.574553</td>\n",
       "      <td>2</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.832317</td>\n",
       "      <td>0.839859</td>\n",
       "      <td>2</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical Parity</th>\n",
       "      <td>0.011532</td>\n",
       "      <td>0.083812</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictive Parity</th>\n",
       "      <td>0.402790</td>\n",
       "      <td>0.110087</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equal Opportunity</th>\n",
       "      <td>0.044670</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Group Difference in False Negative Rate</th>\n",
       "      <td>0.044670</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized Odds</th>\n",
       "      <td>0.084310</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conditional Use Accuracy</th>\n",
       "      <td>0.243624</td>\n",
       "      <td>0.102965</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Group Difference in Accuracy</th>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Treatment Equality</th>\n",
       "      <td>2.299570</td>\n",
       "      <td>0.188965</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimal Group Recall</th>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>2</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 original   updated  \\\n",
       "Accuracy                                         0.838103  0.784137   \n",
       "Balanced Accuracy                                0.822911  0.750949   \n",
       "F1 score                                         0.790698  0.678832   \n",
       "MCC                                              0.666940  0.570736   \n",
       "Precision                                        0.865741  0.905844   \n",
       "Recall                                           0.727626  0.542802   \n",
       "ROC AUC                                          0.905840  0.862754   \n",
       "Statistical Parity                               0.117780  0.042029   \n",
       "Predictive Parity                                0.011405  0.057900   \n",
       "Equal Opportunity                                0.111874  0.019202   \n",
       "Average Group Difference in False Negative Rate  0.111874  0.019202   \n",
       "Equalized Odds                                   0.072012  0.015676   \n",
       "Conditional Use Accuracy                         0.007990  0.055882   \n",
       "Average Group Difference in Accuracy             0.011846  0.019168   \n",
       "Treatment Equality                               0.190827  0.074139   \n",
       "Minimal Group Recall                             0.663636  0.531818   \n",
       "Accuracy                                         0.838997  0.844660   \n",
       "Balanced Accuracy                                0.817959  0.809933   \n",
       "F1 score                                         0.777156  0.765854   \n",
       "MCC                                              0.656419  0.675654   \n",
       "Precision                                        0.838164  0.920821   \n",
       "Recall                                           0.724426  0.655532   \n",
       "ROC AUC                                          0.884706  0.880991   \n",
       "Statistical Parity                               0.065764  0.057681   \n",
       "Predictive Parity                                0.026581  0.003361   \n",
       "Equal Opportunity                                0.032645  0.019629   \n",
       "Average Group Difference in False Negative Rate  0.032645  0.019629   \n",
       "Equalized Odds                                   0.022981  0.014866   \n",
       "Conditional Use Accuracy                         0.026546  0.020709   \n",
       "Average Group Difference in Accuracy             0.008948  0.020596   \n",
       "Treatment Equality                               0.016129  0.006667   \n",
       "Minimal Group Recall                             0.706161  0.644550   \n",
       "Accuracy                                         0.817252  0.886172   \n",
       "Balanced Accuracy                                0.769076  0.775249   \n",
       "F1 score                                         0.625342  0.693046   \n",
       "MCC                                              0.508649  0.647475   \n",
       "Precision                                        0.577441  0.873112   \n",
       "Recall                                           0.681909  0.574553   \n",
       "ROC AUC                                          0.832317  0.839859   \n",
       "Statistical Parity                               0.011532  0.083812   \n",
       "Predictive Parity                                0.402790  0.110087   \n",
       "Equal Opportunity                                0.044670  0.000743   \n",
       "Average Group Difference in False Negative Rate  0.044670  0.000743   \n",
       "Equalized Odds                                   0.084310  0.004072   \n",
       "Conditional Use Accuracy                         0.243624  0.102965   \n",
       "Average Group Difference in Accuracy             0.045250  0.059365   \n",
       "Treatment Equality                               2.299570  0.188965   \n",
       "Minimal Group Recall                             0.655000  0.574257   \n",
       "\n",
       "                                                 classifier  metric_type  \n",
       "Accuracy                                                  0  performance  \n",
       "Balanced Accuracy                                         0  performance  \n",
       "F1 score                                                  0  performance  \n",
       "MCC                                                       0  performance  \n",
       "Precision                                                 0  performance  \n",
       "Recall                                                    0  performance  \n",
       "ROC AUC                                                   0  performance  \n",
       "Statistical Parity                                        0     fairness  \n",
       "Predictive Parity                                         0     fairness  \n",
       "Equal Opportunity                                         0     fairness  \n",
       "Average Group Difference in False Negative Rate           0     fairness  \n",
       "Equalized Odds                                            0     fairness  \n",
       "Conditional Use Accuracy                                  0     fairness  \n",
       "Average Group Difference in Accuracy                      0     fairness  \n",
       "Treatment Equality                                        0     fairness  \n",
       "Minimal Group Recall                                      0     fairness  \n",
       "Accuracy                                                  1  performance  \n",
       "Balanced Accuracy                                         1  performance  \n",
       "F1 score                                                  1  performance  \n",
       "MCC                                                       1  performance  \n",
       "Precision                                                 1  performance  \n",
       "Recall                                                    1  performance  \n",
       "ROC AUC                                                   1  performance  \n",
       "Statistical Parity                                        1     fairness  \n",
       "Predictive Parity                                         1     fairness  \n",
       "Equal Opportunity                                         1     fairness  \n",
       "Average Group Difference in False Negative Rate           1     fairness  \n",
       "Equalized Odds                                            1     fairness  \n",
       "Conditional Use Accuracy                                  1     fairness  \n",
       "Average Group Difference in Accuracy                      1     fairness  \n",
       "Treatment Equality                                        1     fairness  \n",
       "Minimal Group Recall                                      1     fairness  \n",
       "Accuracy                                                  2  performance  \n",
       "Balanced Accuracy                                         2  performance  \n",
       "F1 score                                                  2  performance  \n",
       "MCC                                                       2  performance  \n",
       "Precision                                                 2  performance  \n",
       "Recall                                                    2  performance  \n",
       "ROC AUC                                                   2  performance  \n",
       "Statistical Parity                                        2     fairness  \n",
       "Predictive Parity                                         2     fairness  \n",
       "Equal Opportunity                                         2     fairness  \n",
       "Average Group Difference in False Negative Rate           2     fairness  \n",
       "Equalized Odds                                            2     fairness  \n",
       "Conditional Use Accuracy                                  2     fairness  \n",
       "Average Group Difference in Accuracy                      2     fairness  \n",
       "Treatment Equality                                        2     fairness  \n",
       "Minimal Group Recall                                      2     fairness  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ensemble_preds = ensemble_predict(texts=test_dataset[\"text\"], ensemble=fair_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "single_preds = ensemble_predict(texts=test_dataset[\"text\"], ensemble=fair_ensemble[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_recall=np.float64(0.6547945205479452)\n",
      "equal_opportunity=np.float64(0.027376022087713725)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.dataframe.frame.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
