{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanrystrom/repos/oxonfair/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from joblib import Memory\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    ModelOutput,  # or just use dict if not subclassing\n",
    ")\n",
    "\n",
    "CACHE_DIR = Path().cwd().parent / \".cache\"\n",
    "if not CACHE_DIR.exists():\n",
    "    CACHE_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(CACHE_DIR)\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def load_df(split: str = \"train\") -> pd.DataFrame:\n",
    "    splits = {\n",
    "        \"train\": \"data/train-00000-of-00001.parquet\",\n",
    "        \"test\": \"data/test-00000-of-00001.parquet\",\n",
    "    }\n",
    "    return pd.read_parquet(\n",
    "        \"hf://datasets/FundSciImpact/semeval-2016/\" + splits[split],\n",
    "        columns=[\"Tweet\", \"Target\", \"Sentiment\"],\n",
    "    )\n",
    "\n",
    "\n",
    "df = load_df()\n",
    "df = df[df[\"Sentiment\"] != \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"|                                             |   count |\\n|:--------------------------------------------|--------:|\\n| ('Feminist Movement', 'neg')                |     513 |\\n| ('Hillary Clinton', 'neg')                  |     441 |\\n| ('Legalization of Abortion', 'neg')         |     432 |\\n| ('Atheism', 'pos')                          |     310 |\\n| ('Hillary Clinton', 'pos')                  |     221 |\\n| ('Climate Change is a Real Concern', 'neg') |     196 |\\n| ('Legalization of Abortion', 'pos')         |     188 |\\n| ('Atheism', 'neg')                          |     180 |\\n| ('Climate Change is a Real Concern', 'pos') |     125 |\\n| ('Feminist Movement', 'pos')                |     119 |\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Target\", \"Sentiment\"]].value_counts().to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classes = [\"pos\", \"neg\"]\n",
    "# Assuming Target column contains the classes we want to one-hot encode\n",
    "target_classes = df[\"Target\"].unique().tolist()  # Take first 5 unique values\n",
    "all_classes = sentiment_classes + target_classes\n",
    "# Create dictionaries for mapping\n",
    "sentiment_class2id = {\n",
    "    class_: class_id for class_id, class_ in enumerate(sentiment_classes)\n",
    "}\n",
    "target_class2id = {class_: class_id for class_id, class_ in enumerate(target_classes)}\n",
    "# All class mappings (sentiment + targets)\n",
    "all_classes = sentiment_classes + target_classes\n",
    "class2id = {class_: class_id for class_id, class_ in enumerate(all_classes)}\n",
    "id2class = {class_id: class_ for class_, class_id in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"text\": df[\"Tweet\"].tolist(),\n",
    "    \"sentiment\": df[\"Sentiment\"].tolist(),\n",
    "    \"target\": df[\"Target\"].tolist(),\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# 3. Load tokenizer\n",
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocess function\n",
    "def preprocess_function(example):\n",
    "    # Tokenize text\n",
    "    tokenized = tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "    # Prepare multilabel format\n",
    "    labels = [0.0] * len(all_classes)\n",
    "\n",
    "    # Set sentiment label (binary classification - either pos or neg)\n",
    "    sentiment_id = class2id[example[\"sentiment\"]]\n",
    "    labels[sentiment_id] = 1.0\n",
    "\n",
    "    # Set target labels (one-hot encoding for the 5 target classes)\n",
    "    if example[\"target\"] in target_classes:\n",
    "        target_id = class2id[example[\"target\"]]\n",
    "        labels[target_id] = 1.0\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2180/2180 [00:00<00:00, 10156.26 examples/s]\n",
      "Map: 100%|██████████| 545/545 [00:00<00:00, 10343.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prepare data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 6. Metrics function\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(\n",
    "        predictions=predictions, references=labels.astype(int).reshape(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 7. Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=len(all_classes),\n",
    "    id2label=id2class,\n",
    "    label2id=class2id,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"multilabel_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/m7_xt1cd097bwm56ny1027480000gn/T/ipykernel_49715/1937891243.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "def compute_loss_func(\n",
    "    outputs: ModelOutput | dict,\n",
    "    labels: torch.Tensor,\n",
    "    num_items_in_batch: int,  # noqa: ARG001\n",
    "    num_classification_labels: int = 2,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Custom loss function for HuggingFace Trainer:\n",
    "    - Binary log loss for the first element\n",
    "    - Squared loss (MSE) for the remaining elements\n",
    "\n",
    "    Args:\n",
    "        outputs: ModelOutput or dict containing 'logits' of shape (batch_size, num_outputs)\n",
    "        labels: Tensor of shape (batch_size, num_outputs), ground-truth labels\n",
    "        num_items_in_batch: Total number of items in the accumulated batch (unused here)\n",
    "        num_classification_labels: Number of non-group based classification labels (default: 2)\n",
    "\n",
    "    Returns:\n",
    "        Scalar tensor representing the combined loss\n",
    "    \"\"\"\n",
    "    logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n",
    "\n",
    "    # Binary classification loss for the first outputs\n",
    "    log_loss = F.binary_cross_entropy_with_logits(\n",
    "        logits[:, :num_classification_labels], labels[:, :num_classification_labels]\n",
    "    )\n",
    "\n",
    "    # Regression loss (MSE) for remaining outputs\n",
    "    if logits.shape[1] > 1:\n",
    "        mse_loss = F.mse_loss(\n",
    "            logits[:, num_classification_labels:], labels[:, num_classification_labels:]\n",
    "        )\n",
    "        loss = log_loss + mse_loss\n",
    "    else:\n",
    "        loss = log_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_loss_func=compute_loss_func,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='819' max='819' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [819/819 03:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439806</td>\n",
       "      <td>0.530275</td>\n",
       "      <td>0.530152</td>\n",
       "      <td>0.371145</td>\n",
       "      <td>0.927523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.432666</td>\n",
       "      <td>0.561206</td>\n",
       "      <td>0.546587</td>\n",
       "      <td>0.387779</td>\n",
       "      <td>0.925688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.504109</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>0.553028</td>\n",
       "      <td>0.394295</td>\n",
       "      <td>0.925688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=819, training_loss=0.3579169612227779, metrics={'train_runtime': 204.6023, 'train_samples_per_second': 31.964, 'train_steps_per_second': 4.003, 'total_flos': 143747311387200.0, 'train_loss': 0.3579169612227779, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Function to make predictions\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(\n",
    "        model.device\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    probs = sigmoid(logits)[0]\n",
    "\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "\n",
    "    # Get sentiment prediction\n",
    "    sentiment_idx = np.argmax(probs[: len(sentiment_classes)])\n",
    "    sentiment = sentiment_classes[sentiment_idx]\n",
    "\n",
    "    # Get target predictions (can be multiple)\n",
    "    target_predictions = []\n",
    "    for i, val in enumerate(predictions[len(sentiment_classes) :]):\n",
    "        if val == 1:\n",
    "            target_predictions.append(target_classes[i])\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"targets\": target_predictions,\n",
    "        \"probabilities\": {id2class[i]: float(probs[i]) for i in range(len(probs))},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'neg',\n",
       " 'targets': ['Hillary Clinton',\n",
       "  'Climate Change is a Real Concern',\n",
       "  'Legalization of Abortion',\n",
       "  'Atheism'],\n",
       " 'probabilities': {'pos': 0.04228944703936577,\n",
       "  'neg': 0.9575344324111938,\n",
       "  'Feminist Movement': 0.45859676599502563,\n",
       "  'Hillary Clinton': 0.5194833278656006,\n",
       "  'Climate Change is a Real Concern': 0.6952791810035706,\n",
       "  'Legalization of Abortion': 0.5447657704353333,\n",
       "  'Atheism': 0.5481216907501221}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"The world is burning and we cannot stop the oil!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
